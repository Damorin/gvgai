\documentclass[conference]{IEEEtran}

\usepackage{subfigure,graphicx,url,times,multirow,amsmath,amssymb,algorithm,xspace,epsfig,todonotes,array,caption,color}
\hyphenation{op-tical net-works semi-conduc-tor IEEEtran des-cri-bed}
\usepackage{float}
\usepackage[noend]{algpseudocode}
\usepackage[bookmarks=false]{hyperref}


%\IEEEoverridecommandlockouts

\newcommand{\TODO}[2]{\textcolor{#1}{#2}}
\definecolor{red}{RGB}{255,0,0}
\newcommand{\vect}[1]{\boldsymbol{#1}}
\providecommand{\e}[1]{\ensuremath{\times 10^{#1}}}
\DeclareMathOperator*{\argmax}{arg\,max}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Neuroevolution for General Video Game Playing}

\author{\IEEEauthorblockN{Spyridon Samothrakis, Diego Perez and Simon Lucas}
\IEEEauthorblockA{School of Computer Science and Electronic Engineering\\
University of Essex, Colchester CO4 3SQ, UK\\
ssamot@essex.ac.uk, dperez@essex.ac.uk, sml@essex.ac.uk}
}


% make the title area
\maketitle


\begin{abstract}
%\boldmath
General Video Game Playing (GVGP) allows for the fair evaluation of algorithms and agents, as it minimiz es the ability of an agent to exploit apriori knowledge in the form of game specific heuristics. In this paper we compare four different, but straightforward, evolutionary algorithms in the new learning stream of the GVGP competition. We show that "crappy results"
\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction} \label{sec:intro}


\todo[inline]{We have to say somewhere that this is the first time this is done for GVG}

\todo[inline]{I'm not sure if this is up to date with the experiments actually run}

Learning how to act in unknown environments is often termed the ``Reinforcement Learning'' problem\cite{RL}. The problem encapsulates the core of artificial intelligence and has been studied widely under different contexts. Broadly speaking, an agents tries to maximize some long term notion of utility or reward by selecting appropriate actions at each state. Classic RL assumes that state can somehow be identified by the agent in a unique fashion (i.e. the environment has the Markov Property). 

A possible way of attacking the problem in the general sense Neuroevolution\cite{neurocrap}. Neuroevolution adapts adapts the weights of a local or global function approximator (in the form of a neural network) in order to maximize reward. The appeal of neurovolution over traditional gradient-based Reinforcement kerning methods is two fold. First, the gradient in RL problems might be unstable and/or hard to approximate, thus requiring extensive hyper-parameter tuning in order to get any performance. Secondly, the tournament-ranking schemes used by evolutionary approaches are robust to outliers, as they are effectively calculations of medians rather than means. Thirdly, classic RL algorithms (at least their critic-only versions~\cite{jakollacrap} can be greatly impacted by the lack of a perfect markov state. Sensor aliasing is a known and common problem in RL under function approximation. 

On the other hand the lack of direct gradient information limits the possible size of the parameters of the function approximator to be evolved. If provided with the correct setup and hyperparameters, RL algorithms might be able to perform at superhuman level in a number of hard problems. 

In practical terms, research into RL has often been coupled with the use of strong heuristics. General Game Playing allows for completely arbitrary games, making the easy application of temporal based methods non-trivial. On the other hand, modern evolutionary algorithms require minimum fine-tuning and can help greatly with providing a great baseline. In this paper we perform experiments using evolutionary algorithms to learn actors in the following setup. 

\begin{itemize}
\item CMA-ES using e-greedy policy and linear function approximator 
\item CMA-ES using e-gredy policy and neural network
\item CMA-ES using softmax policy and a linear function approximator
\item CMA-ES using softmax policy and a neural network
\end{itemize}

We will see specifics of this setup later. In order to make this research easy to replicate and interesting in its own right, the environmental substrate is provided by a the new track of the General Game Playing AI Competition. We evaluate our players in this setup. 


The paper is structured as follows: blah
 
\section{Related Research} \label{sec:lit}


\todo[inline]{Everything is so well documented}

\todo[inline]{Include here definitions of GGP, GVGP, first attempts. Also GVGAI competition with the planning track (2014)}

\section{The GVGAI Framework} \label{sec:framework}

This section describes the framework and games employed in this research.


\subsection{The framework} \label{ssec:framework}

The General Video Game Playing competition and framework (GVG-AI) is built on VGDL (Video Game Description Language), a framework designed by Ebner et al.~\cite{Ebner2013} and developed by Tom Schaul~\cite{schaul2013pyvgdl} for GVGP in Python (\textit{py-vgdl}). In VGDL, there are several \textit{objects} that interact in a two-dimensional rectangular space, with associated coordinates, with the possibility of moving and colliding with each other. VGDL defines an ontology that allows the generation of real-time games and levels with simple text files.

The GVG-AI framework is a Java port of py-vgdl, re-engineered for the GVG-AI Competition by Perez et al.~\cite{Perez2015}, which exposes an interface that allows the implementation of controllers that must determine the actions of the player (also referred to as the \textit{avatar}). The VGDL definition of the game is \textbf{not} provided, so it is the responsibility of the agent to determine the nature of the game and the actions needed to achieve victory. 

The original version of this framework provided a \textit{forward model}, that allowed the agent to simulate actions on the current state to devise the consequences of the available actions. In this case, the agents counted on $40ms$ of CPU time to decide an action on each game cycle. This version of the framework was employed by the organizers of the competition to run what is now called the ``planning track'' of the contest, celebrated on the Computational Intelligence in Games (CIG) conference in Dortmund (Germany) in 2014. A complete description of this competition, framework, rules, entries and results has been published in~\cite{Perez2015}.

2015 will feature a new track for this competition, named ``learning track''. In this track, controllers do \textbf{not} have access to forward model, so they cannot foresee the states reached after applying any of the available actions of the game. In contrast, they will be offered with the possibility of playing the game many times, with the aim of allowing the agents to learn how to play each given game. 

Information is given regarding the state of the game to the controllers, at three different stages: initialization, at each game step (\textit{act} call, called repeatedly, until the game is finished) and termination. Table~\ref{tab:inf} summarizes the information given to the agent and at which stages this is passed.


\begin{table*}[!t]
\begin{center}
\begin{tabular}{|m{2.85cm}|m{5cm}|m{5.25cm}|m{0.4cm}|m{0.4cm}|m{0.4cm}|}
%\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Information}  & \textbf{Description} & \textbf{Possible Values} &  \textbf{Init} &  \textbf{Act} &  \textbf{End} \\ 
\hline
\multicolumn{6}{|c|}{\textbf{Game Information}} \\
\hline
\textbf{Score} & Score of the game. & $[0 .. N] \in \mathbb{N}$ & \checkmark & \checkmark & \checkmark \\
\hline
\textbf{Game tick} & Current game tick. & $[0 .. 1000] \in \mathbb{N}$ & \checkmark & \checkmark & \checkmark \\
\hline
\textbf{Game winner} & Indicates the result of the game for the player. & $\left \{ won, lost, ongoing \right \}$ & \checkmark & \checkmark & \checkmark \\
\hline
\textbf{Is game over} & Indicates if the game is over. & $\left \{ true, false \right \}$ & \checkmark & \checkmark & \checkmark \\
\hline
\textbf{World's dimension (width)} & Width of the level (in pixels). & $[0 .. N] \in \mathbb{R}$ & \checkmark &  &   \\
\hline
\textbf{World's dimension (height)} & Height of the level (in pixels). & $[0 .. N] \in \mathbb{R}$ & \checkmark &  &   \\
\hline
\textbf{Block size (\textit{bs})} & Number of pixels each grid cell has. & $[0 .. N] \in \mathbb{N}$ & \checkmark &  &   \\
\hline
\multicolumn{6}{|c|}{\textbf{Actions Information}} \\
\hline
\textbf{Actions} & Available actions of the game. & A subset of $\left \{ nil, up, down, left, right, use \right \}$ & \checkmark &  &   \\
\hline
\multicolumn{6}{|c|}{\textbf{Avatar Information}} \\
\hline
\textbf{Avatar position ($x$)} & $x$ coordinate (in pixels) of the avatar's position. & $[0 .. N] \in \mathbb{R}$ & \checkmark &  \checkmark  & \checkmark  \\
\hline
\textbf{Avatar position ($y$)} & $y$ coordinate (in pixels) of the avatar's position. & $[0 .. N] \in \mathbb{R}$ & \checkmark & \checkmark & \checkmark  \\
\hline
\textbf{Avatar orientation ($x$)} & $x$ coordinate of the avatar's orientation. & $\left \{ -1, 0, 1 \right \}$ & \checkmark & \checkmark &  \checkmark \\
\hline
\textbf{Avatar orientation ($y$)} & $y$ coordinate of the avatar's orientation.. & $\left \{ -1, 0, 1 \right \}$ & \checkmark & \checkmark & \checkmark  \\
\hline
\textbf{Avatar speed} & Speed of the avatar (pixels/tick). & $[0 .. N] \in \mathbb{N}$ & \checkmark & \checkmark & \checkmark  \\
\hline
\textbf{Avatar last action} & Last action played by the avatar. & One of $\left \{ nil, up, down, left, right, use \right \}$  & \checkmark & \checkmark & \checkmark  \\
\hline
\textbf{Avatar resources} & Collections of pairs $<K,V>$, where $K$ is the type of resource and $V$ is the amount owned. & $\left \{ <K_1,V_1>, <K_2,V_2>, ... \right \}; K_i ,V_i \in \mathbb{N}$  & \checkmark & \checkmark & \checkmark  \\
\hline
\multicolumn{6}{|c|}{\textbf{Grid Information (for each type of sprite)}} \\
\hline
\textbf{Sprite type} & Sprite identifier.& $[0 .. N] \in \mathbb{N}$ & \checkmark & \checkmark & \checkmark  \\
\hline
\textbf{Grid Bitmap} & Presence array of the sprite type on the grid. \newline $1$ means presence, $0$ means absence. & Array $\left \{ 0, 1 \right \}$ of size $[width/bs, height/bs]$ & \checkmark & \checkmark & \checkmark  \\
\hline
\end{tabular}
\caption{Information given to the controllers for action decision. The controller of the agent receives this information on the three different calls: \textit{Init} (at the beginning of the game), \textit{Act} (at every game cycle) and \textit{End} (when the game is over). Pieces of information only sent on the \textit{Init} call are constant throughout the game played.}
\label{tab:inf}
\end{center}
\end{table*}


\subsection{Games} \label{ssec:games}

The games employed for this research are the ones from the training set of the GVG-AI competition. Table~\ref{tab:games} describes each one of these games in detail. In order to show the diversity of these games, Table~\ref{tab:gamesComparative} shows the features of each one of them. 


\begin{table*}[!t]
\begin{center}
\begin{tabular}{|m{1.4cm}|m{8cm}|m{6cm}|}
\hline
\textbf{Game}  & \textbf{Description} & \textbf{Score} \\ 
\hline
\textbf{Aliens} & Similar to traditional Space Invaders, Aliens features the player (avatar) in the bottom of the screen, shooting upwards at aliens that approach Earth, who also shoot back at the avatar. The player loses if any alien touches it, and wins if all aliens are eliminated. & \begin{list}{$\bullet$}{\leftmargin=2pt \itemindent=0em} \item $1$ point is awarded for each alien or protective structure destroyed by the avatar. \item $-1$ point is given if the player is hit. \end{list} \\
\hline
\textbf{Boulderdash} & The avatar must dig in a cave to find at least $10$ diamonds, with the aid of a shovel, before exiting through a door. Some heavy rocks may fall while digging, killing the player if it is hit from above. There are enemies in the cave that might kill the player, but if two different enemies collide, a new diamond is spawned.& \begin{list}{$\bullet$}{\leftmargin=2pt \itemindent=0em} \item $2$ points are awarded for each diamond collected, and $1$ point every time a new diamond is spawned. \item $-1$ point is given if the avatar is killed by a rock or an enemy. \end{list} \\
\hline
\textbf{Butterflies} & The avatar must capture butterflies that move randomly around the level. If a butterfly touches a cocoon, more butterflies are spawned. The player wins if it collects all butterflies, but loses if all cocoons are opened. & \begin{list}{$\bullet$}{\leftmargin=2pt \itemindent=0em} \item $2$ points are awarded for each butterfly captured. \end{list} \\
\hline
\textbf{Chase} & The avatar must chase and kill scared goats that flee from the player. If a goat finds another goat's corpse, it becomes angry and chases the player. The player wins if all scared goats are dead, but it loses if is hit by an angry goat. & \begin{list}{$\bullet$}{\leftmargin=2pt \itemindent=0em} \item $1$ point for killing a goat. \item $-1$ point for being hit by an angry goat. \end{list} \\
\hline
\textbf{Frogs} & The avatar is a frog that must cross a road, full of tracks, and a river, only traversable by logs, to reach a goal.  The player wins if the goal is reached, but loses if it is hit by a truck or falls into the water. & \begin{list}{$\bullet$}{\leftmargin=2pt \itemindent=0em} \item $1$ point for reaching the goal. \item $-2$ points for being hit by a truck. \end{list} \\
\hline
\textbf{Missile Command} & The avatar must shoot at several missiles that fall from the sky, before they reach the cities they are directed towards. The player wins if it is able to save at least one city, and loses if all cities are hit. &  \begin{list}{$\bullet$}{\leftmargin=2pt \itemindent=0em} \item $2$ points are given for destroying a missile. \item $-1$ point for each city hit. \end{list} \\
\hline
\textbf{Portals} & The avatar must find the goal while avoiding lasers that kill him. There are many portals that teleport the player from one location to another. The player wins if the goal is reached, and loses if killed by a laser.& \begin{list}{$\bullet$}{\leftmargin=2pt \itemindent=0em} \item $1$ point is given for reaching the goal. \end{list} \\
\hline
\textbf{Sokoban} & The avatar must push boxes so they fall into holes. The player wins if all boxes are made to disappear, and loses when the timer runs out. & \begin{list}{$\bullet$}{\leftmargin=2pt \itemindent=0em} \item $1$ point is given for each box pushed into a hole. \end{list} \\
\hline
\textbf{Survive Zombies} & The avatar must stay alive while being attacked by spawned zombies. It may collect honey, dropped by bees, in order to avoid being killed by zombies. The player wins if the timer runs out, and loses if hit by a zombie while having no honey (otherwise, the zombie dies).& \begin{list}{$\bullet$}{\leftmargin=2pt \itemindent=0em} \item $1$ point is given for collecting one piece of honey, and also for killing a zombie. \item $-1$ point if the avatar is killed, or it falls into the zombie spawn point. \end{list} \\
\hline
\textbf{Zelda} & The avatar must find a key in a maze to open a door and exit. The player is also equipped with a sword to kill enemies existing in the maze. The player wins if it exits the maze, and loses if it is hit by an enemy. & \begin{list}{$\bullet$}{\leftmargin=2pt \itemindent=0em} \item $2$ points for killing an enemy, $1$ for collecting the key, and another point for reaching the door with it. \item $-1$ point if the avatar is killed. \end{list} \\
\hline
\end{tabular}
\caption{Games in the training set of the GVGAI Competition, employed in the experiments of this paper.}
\label{tab:games}
\end{center}
\end{table*}


\begin{table*}[!t]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{\textbf{Game}} & \multirow{2}{*}{\textbf{Score System}} & \multicolumn{3}{c|}{\textbf{NPC Types}} & \multirow{2}{*}{\textbf{Resouces}} & \multicolumn{3}{c|}{\textbf{Terminations}} & \multirow{2}{*}{\textbf{Action Set}}\\ 
\cline{3-5}
 \cline{7-9}
 &  &  Friendly & Enemies & $>1$ type & & Counters & Exit Door & Timeout & \\
 
\hline
\hline
\textbf{Aliens} & I &  & \checkmark &  &  & \checkmark &  & & A2\\
\hline
\textbf{Boulderdash} & I  &  & \checkmark & \checkmark & \checkmark &  & \checkmark & & A0\\
\hline
\textbf{Butterflies} & I & \checkmark &  &  &  & \checkmark &  & & A1\\
\hline
\textbf{Chase} & I & \checkmark & \checkmark &  &  & \checkmark &  & & A1\\
\hline
\textbf{Frogs} & B  &  &  &  &  &  & \checkmark & & A1\\
\hline
\textbf{Missile Command} & I  &  & \checkmark &  &  & \checkmark &  & & A0\\
\hline
\textbf{Portals} & B  &  &  &  &  &  & \checkmark & & A1\\
\hline
\textbf{Sokoban} & I  &  &  &  &  & \checkmark &  & & A1\\
\hline
\textbf{Survive Zombies} & I  & \checkmark & \checkmark &  & \checkmark &  &  & \checkmark & A1\\
\hline
\textbf{Zelda} & I &  & \checkmark &  & \checkmark &  & \checkmark &  & A0\\
\hline
\end{tabular}
\caption{Games feature comparative. All games of the competition are listed, divided into the $3$ game sets: training (first $10$), validation (next $10$) and test (last $10$). Legend: I: Incremental; B: Binary; D:Discontinuous; A0: All moves; A1: Only directional; A2: Left, right and use. Check Section~\ref{ssec:games} for a full description of the meaning of all terms in this table.}
\label{tab:gamesComparative}
\end{center}
\end{table*}


In this table, the \texttt{Score System} indicates how the reward is given to the avatar by means of a numeric score. This system can be either \textbf{binary} (B) or \textbf{incremental} (I). In the former case, the score of the game will always be $0$ until victory is achieved, when the reward will become $1$. In the latter scenario, points are regularly awarded throughout the course of the game.

The second column, \texttt{NPC Types}, indicates the sort of other moving entities that can be found in the game. \textit{Friendly} refers to those Non Player Characters (NPC) that do not harm the player, whereas \textit{Enemies} refer to those NPCs that pose some hazard in the game. Additionally, one game also features more than one type of enemies (\textit{Boulderdash}).

\texttt{Resources} indicates if the game contains sprites that can be picked up by the player or not, and \texttt{Terminations} relate to how the game can end. A game can finish either by creating or destroying one or more sprites of a given type (column \textit{Counters}), reaching a goal (\textit{Exit Door}) or when a timer runs out (\textit{Timeout}). Independently, all games can finish eventually (with a loss) it the maximum number of game steps that can be played, set to $1000$ for all games, is reached. This limit is introduced in order to avoid degenerate players from never finishing the game.

Finally, each one of these games provides a different set of available actions to the player. Those games labelled in Table~\ref{tab:gamesComparative} as $a0$ provide the complete set of actions ($\left \{ nil, up, down, left, right, use \right \}$). $A1$ games are those where the actions are only directional (this is, the complete set without the $use$ action). Finally, $A2$ indicates the subset composed by $\left \{ nil, left, right, use \right \}$.

It is worthwhile highlighting that the variety of these games poses a serious hazard to the playing agents. The learning algorithm needs to learn how the game has to be played in order to achieve victory and maximize score. It is important to realize that the description of the game is not passed to the agent at any time, thus the only way the agent can learn how to play the game is to analyze the game state (maybe by extracting features) and build relations between the observations and the games played.

Of course, one could try to identify the game that is being played and act consequently. However, this is of a limited research interest, as the long term objective is to find agents that can learn any game (rather than building an agent that uses a database of known games, heuristically prepared to play these).  Figure~\ref{fig:games} shows screenshots of four of the games employed in this research.

\todo[inline]{Maybe I can put a different picture here, not to be repetitive with last CIG.}

\begin{figure*}[!t]
\begin{center}
\includegraphics[width=1.8\columnwidth]{img/games}
\end{center}
\caption{Four of the ten training set games: from top to bottom, left to right, \textit{Butterflies}, \textit{Portals}, \textit{Sokoban} and \textit{Zelda}.}
\label{fig:games}
\end{figure*}




\section{Background} \label{sec:fastEvo}

\todo[inline]{Everything is so powerful}


\subsection{Approaches} \label{sec:fastEvoMem}

\todo[inline]{Everything is so cool}


\section{Experiments} \label{sec:exp}

\todo[inline]{Everything took so long}



\section{Conclusions} \label{sec:conc}

\todo[inline]{Everything is so pretty}

\todo[inline]{Future work: deeper NN, EA, More games. Learn to play in different levels.}

% use section* for acknowledgement
 
\section*{Acknowledgment}


\noindent This work was supported by EPSRC grant EP/H048588/1.

\bibliographystyle{IEEEtran} 
\bibliography{biblio}


% that's all folks
\end{document}


